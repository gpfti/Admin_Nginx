
# Escalado vertical y horizontal en NGINX

NGINX es un servidor web de alto rendimiento, utilizado tanto como servidor HTTP como proxy inverso y balanceador de carga. A diferencia de Apache, que históricamente ha usado múltiples procesos e hilos para manejar conexiones, **NGINX se basa en un modelo asíncrono y orientado a eventos**, lo que le da una gran eficiencia en el manejo de conexiones concurrentes.

## 1. Escalado Vertical (Scale-Up) en NGINX

Consiste en mejorar el rendimiento del servidor NGINX **aumentando los recursos del sistema** (CPU, RAM, I/O) en un único nodo. En este modelo, se ajusta la configuración interna de NGINX para aprovechar mejor esos recursos adicionales.

### Claves del escalado vertical en NGINX

A diferencia de Apache, que trabaja con MPMs y múltiples procesos con hilos, **NGINX utiliza un conjunto fijo de procesos de trabajo (“workers”)** y eventos no bloqueantes para manejar muchas conexiones simultáneamente. Por tanto, el escalado vertical implica:

* Aumentar el número de procesos `worker_processes` si hay más núcleos disponibles.
* Ajustar `worker_connections` para permitir más conexiones por proceso.
* Sintonizar buffers, cachés y tiempos de espera.
* Ampliar recursos físicos (RAM, CPU) en la máquina donde corre NGINX.

---

### Parámetros fundamentales de escalado vertical en NGINX

Archivo de configuración: normalmente en `/etc/nginx/nginx.conf`

```nginx
worker_processes auto;   # Número de procesos de trabajo, idealmente igual al número de núcleos de CPU
worker_rlimit_nofile 100000;

events {
    worker_connections 4096;   # Conexiones máximas por proceso
    use epoll;                 # Modelo de eventos eficiente para Linux
    multi_accept on;           # Procesa múltiples conexiones en cada ciclo
}
```

#### Descripción de los parámetros clave

| Parámetro              | Descripción                                                                                                          |
| ---------------------- | -------------------------------------------------------------------------------------------------------------------- |
| `worker_processes`     | Número de procesos de trabajo. Lo habitual es `auto`, que adapta a los núcleos disponibles.                          |
| `worker_connections`   | Conexiones simultáneas permitidas por proceso. Multiplicado por `worker_processes` define el máximo teórico.         |
| `worker_rlimit_nofile` | Límite del número de archivos abiertos que NGINX puede manejar. Ajusta este valor si se necesitan muchas conexiones. |
| `use epoll`            | Modelo de eventos altamente eficiente en Linux. NGINX lo selecciona automáticamente si está disponible.              |
| `multi_accept`         | Permite a los workers aceptar múltiples conexiones a la vez, mejorando la concurrencia.                              |

---

### Escenario de escalado vertical sin cambiar hardware físico

#### Supuesto inicial

NGINX desplegado en una máquina virtual con:

* 2 vCPUs
* 2 GB de RAM
* worker\_processes = 2
* worker\_connections = 1024

Rendimiento bajo carga con `wrk` o `ab`:

* Saturación a 1500 conexiones simultáneas
* Tiempo de respuesta > 700 ms
* CPU en 90–100%

#### Escalado vertical

Ampliación de recursos desde el hipervisor:

* RAM: de 2 a 8 GB
* vCPU: de 2 a 8

Reconfiguración de NGINX:

```nginx
worker_processes 8;
worker_rlimit_nofile 200000;

events {
    worker_connections 8192;
    multi_accept on;
    use epoll;
}
```

Ahora el servidor puede manejar hasta:

`8 procesos × 8192 conexiones = 65.536 conexiones concurrentes (teóricas)`

#### Validación

1. **Reinicia NGINX**:

   ```bash
   sudo systemctl restart nginx
   ```

2. **Verifica los procesos activos**:

   ```bash
   ps -ef | grep nginx
   ```

3. **Prueba de carga**:

   ```bash
   wrk -t8 -c10000 -d30s http://localhost/
   ```

4. **Monitorea uso de CPU y RAM**:
   Usa `htop` o `atop` para ver si se están aprovechando los núcleos y la memoria.

---

### Resultados esperados

| Métrica                   | Antes (limitado)  | Después (escalado)  |
| ------------------------- | ----------------- | ------------------- |
| vCPU                      | 2                 | 8                   |
| RAM                       | 2 GB              | 8 GB                |
| worker\_connections       | 1024              | 8192                |
| Total conexiones teóricas | 2048              | 65.536              |
| Tiempo de respuesta medio | > 700 ms          | < 150 ms            |
| Saturación                | \~1500 conexiones | \~20.000 conexiones |

---

### 2. Escalado Horizontal en NGINX

Aunque el foco es el escalado vertical, es importante contextualizar brevemente el escalado horizontal en NGINX.

#### ¿Qué es?

Consiste en añadir múltiples instancias de NGINX (en distintos nodos) que compartan la carga entrante a través de un **balanceador de carga externo** o mediante mecanismos como **DNS round-robin, HAProxy, o NGINX en modo reverse proxy balanceador**.

#### Tipos de balanceo en NGINX

NGINX puede actuar él mismo como balanceador horizontal de backend:

```nginx
http {
    upstream backend {
        server app1.local;
        server app2.local;
        server app3.local;
    }

    server {
        location / {
            proxy_pass http://backend;
        }
    }
}
```

Este enfoque es útil en arquitecturas distribuidas, donde NGINX reparte tráfico hacia múltiples nodos de aplicación (por ejemplo, instancias de Tomcat, Node.js o Django).

---

### Punto Único de Fallo (SPOF)

En el contexto de escalado vertical, el servidor NGINX sigue siendo un **SPOF** si no se ha desplegado una estrategia de alta disponibilidad (por ejemplo, con keepalived o failover entre balanceadores). Aunque escales verticalmente y tu servidor tenga más recursos, **si falla, todo cae**.

---

### Conclusión sobre el escalado vertical en NGINX

| Ventajas                                                           | Desventajas                                                |
| ------------------------------------------------------------------ | ---------------------------------------------------------- |
| Alta eficiencia en uso de CPU y RAM con pocos procesos             | Límite físico del servidor único                           |
| Fácil de configurar con `worker_processes` y `worker_connections`  | Sigue siendo un SPOF si no hay redundancia                 |
| Ideal para entornos con alto tráfico pero infraestructura limitada | Escalabilidad limitada frente a arquitecturas distribuidas |

---
## Instalación de wrk para pruebas de carga

**`wrk`**, es una herramienta de benchmarking HTTP muy eficiente y adecuada para probar el rendimiento de NGINX bajo carga.  

Para instalar **`wrk`** en sistemas basados en Ubuntu, los pasos específicos son:

---

## Paso 1: Instalar dependencias necesarias

```bash
sudo apt update
sudo apt install build-essential libssl-dev git -y
```

---

## Paso 2: Clonar el repositorio de `wrk`

```bash
git clone https://github.com/wg/wrk.git
cd wrk
```

---

## Paso 3: Compilar `wrk`

```bash
make
```

Esto generará un ejecutable en el mismo directorio llamado `wrk`.

---

## Paso 4: Instalar (opcional, para tenerlo en el PATH)

```bash
sudo cp wrk /usr/local/bin/
```

Ahora puedes ejecutarlo desde cualquier lugar con:

```bash
wrk --version
```

---

## Comprobación rápida

Lanza una prueba simple contra tu servidor local:

```bash
wrk -t4 -c200 -d30s http://localhost/
```

Esto simula:

* `-t4`: 4 hilos (uno por núcleo si tienes 4 vCPUs)
* `-c200`: 200 conexiones simultáneas
* `-d30s`: duración de 30 segundos

---

## ACTIVIDAD PRACTICA
## Escalado vertical en NGINX

Vamos a construir una actividad práctica completa para NGINX que te permita **simular un entorno limitado**, aplicar un **escalado vertical sin cambiar de máquina**, y luego **validar el impacto con pruebas de carga y monitorización**.

### Escenario

Vamos a simular un servidor NGINX que sirve contenido estático (`index.html`) bajo alta concurrencia. Inicialmente estará limitado a una configuración baja, y luego se aplicará una configuración optimizada para reflejar un entorno escalado verticalmente.

---

## Fase 1: Configuración limitada (bajo rendimiento)

### 1. Configuración del archivo `/etc/nginx/nginx.conf`

```nginx
user www-data;
worker_processes 1;
worker_rlimit_nofile 10240;

events {
    worker_connections 512;
    use epoll;
    multi_accept off;
}

http {
    include       mime.types;
    default_type  application/octet-stream;

    sendfile        on;
    keepalive_timeout  15;

    server {
        listen       80;
        server_name  localhost;

        location / {
            root   /var/www/html;
            index  index.html;
        }
    }
}
```

### 2. Página de prueba

Guarda en `/var/www/html/index.html`:

```html
<!DOCTYPE html>
<html lang="es">
<head>
  <meta charset="UTF-8">
  <title>Prueba de carga</title>
</head>
<body>
  <h1>Servidor NGINX - Configuración limitada</h1>
  <p>Esta página se usa para pruebas de rendimiento.</p>
</body>
</html>
```

### 3. Reinicia y valida

```bash
sudo systemctl restart nginx
curl http://localhost/
```

---

### 4. Prueba de carga (limitada)

Con `wrk`:

```bash
wrk -t4 -c500 -d30s http://localhost/
```

#### Qué esperar:

* Saturación a \~400–500 conexiones.
* Tiempo medio de respuesta elevado.
* CPU con uso alto si el número de conexiones excede los 512 permitidos.

#### Resultados obtenidos de aplicar el test de la carga:
```bash
wrk -t4 -c500 -d30s http://localhost/
Running 30s test @ http://localhost/
  4 threads and 500 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency   171.14ms  271.01ms   1.67s    81.93%
    Req/Sec    14.29k    10.58k   46.14k    65.83%
  1375801 requests in 30.89s, 636.29MB read
Requests/sec:  44541.98
Transfer/sec:     20.60MB
```


### Resumen del escenario

Comando ejecutado:

```bash
wrk -t4 -c500 -d30s http://localhost/
```

* `-t4`: 4 hilos de carga
* `-c500`: 500 conexiones simultáneas
* `-d30s`: duración de la prueba, 30 segundos

---

### Análisis de los resultados

#### 1. **Latencia**

```
Latency   Avg      Stdev     Max   +/- Stdev
          171.14ms  271.01ms   1.67s    81.93%
```

* **Latencia media (Avg)**: 171.14 ms → Es un tiempo de respuesta moderado-alto.
* **Desviación estándar (Stdev)**: 271.01 ms → Muy alta. Hay **gran variabilidad** en los tiempos de respuesta.
* **Máximo (Max)**: 1.67 segundos → Hay peticiones que tardan mucho en responder.
* **81.93% +/- Stdev**: El 82% de las peticiones están dentro del rango de una desviación estándar, lo que indica que hay un grupo importante de respuestas fuera del comportamiento normal (colas, saturación).

**Interpretación**:
El servidor **comienza a saturarse** y muchas conexiones deben esperar, lo cual explica las latencias elevadas y la variabilidad. Esto es típico en una configuración de bajo rendimiento, donde el número de workers o conexiones por worker es insuficiente.

---

#### 2. **Rendimiento por hilo**

```
Req/Sec    14.29k    10.58k   46.14k    65.83%
```

* **Media**: cada hilo maneja unas 14.290 peticiones por segundo.
* **Máximo**: uno de los hilos llegó a gestionar 46.140 req/s.
* **Desviación estándar alta**: 10.580 → Hay gran disparidad en la carga por hilo.
* **65.83% +/- Stdev**: Indica que la distribución de carga entre hilos no es óptima.

**Interpretación**:
El modelo asíncrono de NGINX maneja bien las conexiones, pero con la configuración limitada, no puede distribuir homogéneamente la carga entre los procesos. Algunos hilos trabajan mucho más que otros, posiblemente porque hay pocos `worker_processes`.

---

#### 3. **Totales**

```
1375801 requests in 30.89s, 636.29MB read
Requests/sec:  44541.98
Transfer/sec:     20.60MB
```

* **1.375.801 peticiones servidas en 30,89 segundos** → volumen alto, incluso con limitaciones.
* **44.541 req/s promedio global** → muestra que, aunque limitado, NGINX es extremadamente eficiente.
* **20,60 MB/s de transferencia** → buen throughput para contenido estático.

**Interpretación**:
Aunque la configuración es limitada, NGINX logra mantener un ritmo de **más de 44.000 peticiones por segundo**, lo cual es excelente. Pero el coste en latencia es elevado, indicando que **el sistema está en el límite**: sigue cumpliendo, pero responde más lento.

---

### Conclusiones

| Indicador       | Valor           | Interpretación                                   |
| --------------- | --------------- | ------------------------------------------------ |
| Latencia media  | 171 ms          | El sistema empieza a saturarse                   |
| Latencia máxima | 1.67 s          | Algunas peticiones esperan demasiado             |
| Desviación alta | 271 ms          | Gran variabilidad, no hay consistencia           |
| Req/s total     | 44.541          | Excelente capacidad, incluso en entorno limitado |
| Uso de hilos    | Ineficiente     | Algunos hilos mucho más cargados que otros       |
| Saturación      | Moderada a alta | El sistema aún responde, pero al límite          |

### Diagnóstico final:

Este es un **entorno subóptimo** que demuestra cómo NGINX sigue siendo eficaz con pocos recursos, pero con claras señales de estrés bajo carga. Este resultado es ideal como base para contrastar con la siguiente prueba que harás con la **configuración optimizada y escalado vertical completo**.


---

## Fase 2: Configuración optimizada (post-escalado vertical)

Supongamos que has ampliado desde el hipervisor de 2 a 8 vCPUs, y de 2 GB a 8 GB de RAM.

### 1. Nueva configuración optimizada en `/etc/nginx/nginx.conf`

```nginx
user www-data;
worker_processes 8;
worker_rlimit_nofile 200000;

events {
    worker_connections 8192;
    use epoll;
    multi_accept on;
}

http {
    include       mime.types;
    default_type  application/octet-stream;

    sendfile        on;
    keepalive_timeout  15;

    server {
        listen       80;
        server_name  localhost;

        location / {
            root   /var/www/html;
            index  index.html;
        }
    }
}
```

### 2. Reinicia NGINX

```bash
sudo systemctl restart nginx
```

### 3. Verifica procesos activos

```bash
ps -ef | grep nginx
```

Deberías ver 1 proceso maestro + 8 workers.

---

### 4. Prueba de carga (optimizada)

```bash
wrk -t8 -c20000 -d60s http://localhost/
```

### 5. Monitoriza con `htop`, `vmstat`, o `atop`

Mira cómo se distribuye el uso de CPU entre los núcleos y cómo responde el sistema bajo alta concurrencia.

---

## Comparativa esperada

| Métrica                      | Configuración limitada | Configuración optimizada |
| ---------------------------- | ---------------------- | ------------------------ |
| `worker_processes`           | 1                      | 8                        |
| `worker_connections`         | 512                    | 8192                     |
| Conexiones concurrentes max. | \~512                  | \~65536                  |
| Tiempo de respuesta medio    | > 700 ms               | < 150 ms                 |
| Errores HTTP 503 esperados   | Posibles               | Poco probables           |
| Uso de CPU                   | Saturación rápida      | Distribución eficiente   |
| Adecuado para producción     | No                     | Sí                       |

---

## Interpretación

* La configuración limitada es ideal para simular cuellos de botella y observar la saturación del servidor.
* La configuración optimizada demuestra cómo NGINX escala **verticalmente** usando más recursos de hardware sin cambiar la arquitectura.
* La eficiencia de NGINX en eventos y procesos ligeros permite manejar decenas de miles de conexiones concurrentes con recursos relativamente modestos.


[Vamos al siguiente contenido](./10-K.md)
